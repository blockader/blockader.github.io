

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/null">
  <link rel="icon" href="/null">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Shuqin Xie, Dongfeng Yu, Raymond Lau">
  <meta name="keywords" content="">
  
  <title>Dense Token Supervision for Accelerating Transformer Training</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.10","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 16vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>
        Dense Token Supervision for Accelerating Transformer Training
      </strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
      data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
      aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/images/gray.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="">
              
                
              
            </span>

            
              <div class="mt-3">
  
      
        
</div>

<div class="mt-1">
  

        

              
                
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none"></h1>
            
            <div class="markdown-body">
              <p><strong><h2 style="text-align:center;">Dense Token Supervision for Accelerating Transformer Training</h2></strong></p>
<p style="text-align:center;">
    Shuqin Xie, Dongfeng Yu, Raymond Lau
    <br>
    Carnegie Mellon University
</p>


<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h2><p>Transformer-based architectures are getting more and more attention in the computer vision field starting from their successful application in image classification[1,12]. The recent proposed Vision Transformer [1] (ViT) divides an image into a sequence of patches and considers them as input tokens to a transformer. It relies on a “class token” to generate an image-level prediction and only applies loss on this token. The token-wise nature of the vision transformer raises the question of whether we can apply supervision on every token to serve purposes such as achieving better accuracy or more efficient training. Our project explores this idea of dense token supervision for the transformer on <strong>video action recognition</strong> and <strong>image classification tasks</strong>. The following figure depicts dense token supervision.<br><img align="center" width="1400" src="/images/intro1.gif" srcset="/img/loading.gif" lazyload></p>
<p>We first consider the video action recognition task. FineGym[2] dataset provides both video-level labels and frame-level labels, which is perfectly suitable for our project as we can consider each frame as one token and naturally obtain labels for tokens. Our takeaway from this task is that applying dense token supervision can decrease training loss much faster. But since FineGym is small and is relatively easy, we don’t observe obvious performance differences after training for one epoch. We then move to the image classification task, where larger datasets are available (e.g. ImageNet). Unlike the video experiment, there is no available labels for image patches. Here we use a pre-trained CNN model to generate token labels for free. Our results show that applying dense token supervision to transformers makes it easier for the model to converge to satisfying accuracy (less sensitive to the learning rate), and accelerates the training when trained from scratch.</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a><strong>Related Work</strong></h2><p><strong>Vision Transformer</strong> DeTR [3], which is achieving similar performance with other state-of-the-art object detection algorithms, is a novel architecture composed of traditional CNN and transformer. ViT [1] is an image classifier based on the transformer and consumes a divided image of 16 x 16 words. DeiT [4] further improves ViT’s data efficiency by distilling knowledge using a CNN. [5] extends ViT from image classification to action recognition.<br><strong>Action Recognition</strong> Action recognition models take videos as input and predict the corresponding actions (labels). Recent works [6, 7, 8] utilize 2D CNNs structure to extract framewise features and further combined those with a temporal model. It is an improvement compared to the 3D convolution neural networks [9], which has high complexity and thus computational expensive.<br><strong>Fine-grained Action Recognition</strong> State-of-the-art models have achieved great success in classical action recognition. The focus has been shifted to fine-grained action recognition [2,7]. For fine-grained action recogition, some classes are similar in context but different in some sub-action levels. Compared to traditional action recognition, models are required to differentiate those minor differences and learn the specific temporal and spatial features. [11] is one of the models, which is based on attention mechanism and semi-supervised learning.</p>
<h2 id="Video-Classification-with-Frame-level-Labels"><a href="#Video-Classification-with-Frame-level-Labels" class="headerlink" title="Video Classification with Frame-level Labels"></a><strong>Video Classification with Frame-level Labels</strong></h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p><strong>FineGym</strong> [2] is an action recognition dataset that contains coarse-to-fine annotations with three levels of labels. The data source is based on the gymnastic videos, which are publicly available on YouTube. The top-level corresponds to events such as balance beam or uneven bars. The second level and the third level corresponds to the sub-actions with decreasing granularity.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>We extend vision transformer into video transformer as follows. Given a video with L frames, we divide each frame into 14x14 patches and feed them into the transformer architecture. With an extra class token, there are Lx14x14+1 tokens in total. Token features from the same frame are fused to generate a final logit for frame-level prediction. Since frame-level labels on FineGym is category label, we apply cross-entropy loss on each frame logit. The architecture is shown below. To utilize pre-trained image classification models, we initialize the position embedding by replicating pre-trained model’s position embedding L times. The other weights are copied from pre-trained models.<br><img align="center" width="1200" src="/images/video_transformer.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>As we mentioned before, this dataset is too easy that ViT, and our approach’s performance almost saturate after one epoch. In terms of top 1 accuracy, we did not observe any significant difference. But we found at training time, with dense token supervision, video classification loss decrease much faster than baseline. This leads to our image classification experiments. </p>
<table>
<thead>
<tr>
<th></th>
<th>Top-1 acc</th>
</tr>
</thead>
<tbody><tr>
<td>ViT</td>
<td>0.985</td>
</tr>
<tr>
<td>Ours</td>
<td>0.990</td>
</tr>
</tbody></table>
<h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a><strong>Image Classification</strong></h2><h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h3><center>
<img align="center" width="1200" src="/images/dense_patch.png" srcset="/img/loading.gif" lazyload>
</center>


<p>The above figure illustrates our model, which densely applies losses on every token (patch). To obtain token labels for free, we utilize a ResNet50 model pre-trained on ImageNet. It works as follows.<br>We remove the global average pooling layer after conv5 features, convert the FC layer into a 1x1 conv layer and directly apply it to conv5 features. In this way, instead of getting one output vector, we obtain a map of predictions where each bin represents the network’s belief on one patch. We can then use them as labels to train token outputs.<br>For example, given an input of size 224 x 224, we obtain an output of size 7 x 7 x 1000, each bin is a 1000-class probability distribution. Since they are probability distributions, we use KL-divergence as the loss function. This process is shown in the figure below.<br>This approach can be related to knowledge distillation, where we distillate a pre-trained CNN’s believes on different patches to the transformer’s token features. Note that we add a 2x2 conv layer with stride 2 on top of the token features so that the output size matches the label size. </p>
<center>
<img align="center" height="300" src="/images/cnn_labeling.png" srcset="/img/loading.gif" lazyload>
</center>

<h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p><strong>Finetuning</strong> We begin with finetuning pre-trained ViT models on ImageNet 1k dataset. We experiment with 2 learning rates (1e-3 and 6.25e-5) and whether apply dense supervision (DS). Each experiment is trained for 8 epochs and under the same hyperparameters; results are summarized in the table below.<br>Our finding is interesting. Both approaches perform well under a small learning rate (6.25e-5), but a performance gap occurs when using a larger learning rate (1e-3). Dense supervision performs much better than the vanilla ViT model in this setting. This suggests that on the one hand, using a large learning rate will mess up with the original pre-trained weights while a small lr does not. This accounts for the performance gap of the vanilla ViT models. On the other hand, dense token supervision helps recover from the bad weights, accounting for the gap between ViT and ViT w DS.<br>This suggests maybe this technique can be used for training from scratch since all weights are initialized randomly.<br><strong>Training from scratch</strong> We then train ViT and ViT w/DS from scratch to verify the above hypothesis. Similarly, we train both models for 8 epochs with the same hyperparameters. Results are shown in the table below. As expected, with dense supervision, the model performs much better than the ViT model. In fact, we observe our model almost achieves 2x speedup compared to vanilla ViT. This suggests our approach can be used to accelerate ViT training.<br><strong>Comparison with DeiT</strong> DeiT [12] also distillates knowledge from a teacher network. It differs from our approach in that it doesn’t consider dense token(patch) supervision. Instead, it introduces another “dist_token” and trains it to match with the hard label from the teacher network. We train DeiT using the same setting as our ViT model and ViT w/DS model and report results in the table below. We can see that DeiT performs slightly better than ViT but is still far behind ViT w DS. This suggests our approach is more than just knowledge distillation. Dense supervision can accelerate training a lot.</p>
<table>
<thead>
<tr>
<th>Finetuning</th>
<th>lr</th>
<th>Top-1 Acc</th>
<th>Top-5 Acc</th>
</tr>
</thead>
<tbody><tr>
<td>Vanilla ViT</td>
<td>6.25e-5</td>
<td>73.5</td>
<td>90.8</td>
</tr>
<tr>
<td>ViT w/ DS</td>
<td>6.25e-5</td>
<td>74.1</td>
<td>91.3</td>
</tr>
<tr>
<td>Vanilla ViT</td>
<td>1e-3</td>
<td>49.0</td>
<td>73.0</td>
</tr>
<tr>
<td>ViT w/ DS</td>
<td>1e-3</td>
<td>73.2</td>
<td>91.4</td>
</tr>
<tr>
<td>DeiT</td>
<td>1e-3</td>
<td>51.5</td>
<td>75.1</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Train from scratch</th>
<th>lr</th>
<th>Top-1 Acc</th>
<th>Top-5 Acc</th>
</tr>
</thead>
<tbody><tr>
<td>Vanilla ViT</td>
<td>6.25e-5</td>
<td>48.8</td>
<td>73.2</td>
</tr>
<tr>
<td>ViT w/ DS</td>
<td>6.25e-5</td>
<td>60.3</td>
<td>82.5</td>
</tr>
</tbody></table>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>In this project, we propose to densely supervise tokens to accelerate vision transformer training. We extend vision transformer into video transformer for action recognition task and utilize a pre-trained CNN to generate free token labels for the image classification task. Our results show that dense token supervision provides a 2x speedup compared to regular ViT.</p>
<h2 id="Supplemental-Materials"><a href="#Supplemental-Materials" class="headerlink" title="Supplemental Materials"></a><strong>Supplemental Materials</strong></h2><h3 id="Presentation-Video"><a href="#Presentation-Video" class="headerlink" title="Presentation Video"></a>Presentation Video</h3><style>.embed-container {
    position: relative;
    padding-bottom: 56.25%;
    height: 0;
    overflow: hidden;
    max-width: 100%;
  }
  .embed-container iframe, .embed-container object, .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }
  </style>

<div class="embed-container"><iframe src="https://www.youtube.com/embed/Ks_YPSJZAGE" allowfullscreen frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe></div>

<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>Our code is available at <a target="_blank" rel="noopener" href="https://github.com/xieshuqin/DenseTokenSupervision">https://github.com/xieshuqin/DenseTokenSupervision</a>. Since each experiment takes 1.5 days on a single GPU machine, we provide our Tensorboard logs <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1VI98J_tzPtN79iLnzw4QbdVg-WmOreZp?usp=sharing">here</a>.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h2><p>[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.<br>[2] Shao, Dian, et al. “Finegym: A hierarchical video dataset for fine-grained action understanding.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and SergeyZagoruyko. End-to-end object detection with trans-formers. In European Conference on Computer Vision, pages 213–229. Springer, 2020.<br>[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-cisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers &amp; distillation through attention. arXiv preprint arXiv:2012.12877, 2020.<br>[5] Daniel Neimark, Omri Bar, Maya Zohar, and DotanAsselmann.Video transformer network. arXiv preprint, arXiv:2102.00719, 2021.<br>[6] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on Computer Vision, pages 20–36. Springer, 2016<br>[7] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, 2019<br>[8] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 6202–6211, 2019.<br>[9] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-resani, and Manohar Paluri. Learning spatio-temporal features with 3d convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4489–4497, 2015.<br>[10] Jihoon Chung, Cheng hsin Wuu, Hsuan Ru Yang, Yu-Wing  Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos, 2020.<br>[11] Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, and Chi-Keung Tang. Semi-supervised few-shot atomic action recognition.arXiv preprint arXiv:2011.08410, 2020.<br>[12] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention. arXiv preprint arXiv:2012.12877, 2020.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header">&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
    
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>








  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>



<!DOCTYPE html>
<html lang="en" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/null">
  <link rel="icon" href="/null">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Shuqin Xie, Dongfeng Yu, Raymond Lau">
  <meta name="keywords" content="">
  
  <title>Dense Token Supervision for Accelerating Transformer Training</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.10","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 16vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>
        Dense Token Supervision for Accelerating Transformer Training
      </strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
      data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
      aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">Light</a>
          </li>
          
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/images/gray.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="">
              
                
              
            </span>

            
              <div class="mt-3">
  
      
        
</div>

<div class="mt-1">
  

        

              
                
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none"></h1>
            
            <div class="markdown-body">
              <p><strong><h2 style="text-align:center;">Dense Token Supervision for Accelerating Transformer Training</h2></strong></p>
<p style="text-align:center;">
    Shuqin Xie, Dongfeng Yu, Raymond Lau
    <br>
    Carnegie Mellon University
</p>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h2><p>Transformer-based architectures are getting more and more attention in the computer vision field starting from its successful application in image classification[1]. Such architectures consider an image as a sequence of patches and use multiple layers of self-attention and MLP to generate representation for each token. Its token-wise nature raises the question of whether we can apply dense supervision on each of the token to serve purposes such as achieving better accuracy or more efficient training.<br><img align="center" width="1024" height="512" src="/images/transformer.png" srcset="/img/loading.gif" lazyload><br>Our project explores this idea of dense token supervision for transformer on video action recognition and image classification tasks. We first consider the video action recognition task as the research interest in this area is gradually shifting to fine-grained action recogntion tasks to deal with the huge gap between the success of the state of the art action recogniton models and real-word applications. FineGym[2] is a fine-grained action recogniton dataset that provides multiple levels of supervision for a single video, which is pefectly suitable for our project as we can consider each frame as one token. We also consider the case for single image classification, where we use a pretrained teacher model to provide patch-wise supervision. Our results show that applying dense token supervision to transformers makes it easier for the model to converge to satifying accuracy (less sensensitive to the learning rate), and accelerates the training when trained from scratch.</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a><strong>Related Work</strong></h2><p><strong>Vision Transformer</strong> DeTR [3] combines transformer and CNN for object detection, achieving comparable performance with mature detection algorithms.<br>ViT [1] divides an image to 16 x 16 words and successfully uses a transformer to classify the image. DeiT [4] improves ViT’s data efficiency by distill knowledge from a CNN. [5] extends ViT to action recognition.<br><strong>Action Recognition</strong> Video action recognition models pre-dict the label (action) of a given video. Instead of using 3D convolution neural networks [6], which is simple yet com-putationally expensive, recent works [7, 8, 9] utilize 2D CNNs as the frame-wise feature extractor, combined with acarefully designed temporal model.<br><strong>Fine-grained Action Recognition</strong> Having achieved high performance on classical datasets, the area of action recognition shifts to the fine-grained ones [2,8],  where some classes are similar in context and different in only sub-action level, and the models have to learn robust spatial andtemporal features to solve the task. Existing work [11] applies semi-supervised learning and attention-based feature aggregation to better address the hard datasets.  </p>
<h2 id="Video-Classification-with-Frame-level-Labels"><a href="#Video-Classification-with-Frame-level-Labels" class="headerlink" title="Video Classification with Frame-level Labels"></a><strong>Video Classification with Frame-level Labels</strong></h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p><strong>FineGym</strong> [2] is an action recognition dataset that provides coarse-to-fine annotations with its three levels of labels. It is built on top of gymnastic videos available on YouTube. The top level corresponds to events, such as balance-beam or uneven-bars, and the second level and the third level describe the sub-actions with decreasing granularity. The richness, quality and diversity of this dataset propose new challenges to the area.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>Introduce our model.<br><img align="center" width="1024" height="512" src="/images/video.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Show the results.</p>
<h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a><strong>Image Classification</strong></h2><h3 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a>Dataset</h3><p>Introduce ImageNet and Places365.</p>
<h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h3><p>Introduce our model.<br><img align="center" width="1024" height="512" src="/images/image.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p>Show the results.<br><img align="center" width="1024" height="512" src="/images/pretrained.png" srcset="/img/loading.gif" lazyload><br><img align="center" width="1024" height="512" src="/images/scratch.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>Give an overall conclusion, and talk about future steps.</p>
<h2 id="Supplemental-Materials"><a href="#Supplemental-Materials" class="headerlink" title="Supplemental Materials"></a><strong>Supplemental Materials</strong></h2><h3 id="Presentation-Video"><a href="#Presentation-Video" class="headerlink" title="Presentation Video"></a>Presentation Video</h3><div class="video-container"><iframe   src=https://www.youtube.com/embed/Ks_YPSJZAGE?rel=0&amp;showinfo=0 frameborder=0 gesture=media allow=encrypted-media allowfullscreen></iframe></div>

<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p><a href="/files/dense_token.zip">dense_token.zip</a></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h2><p>[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.<br>[2] Shao, Dian, et al. “Finegym: A hierarchical video dataset for fine-grained action understanding.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and SergeyZagoruyko. End-to-end object detection with trans-formers. In European Conference on Computer Vi-sion, pages 213–229. Springer, 2020.<br>[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-cisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers &amp; distillation through attention. arXiv preprint arXiv:2012.12877, 2020.<br>[5] Daniel Neimark, Omri Bar, Maya Zohar, and DotanAsselmann.Video transformer network. arXiv preprint, arXiv:2102.00719, 2021.<br>[6] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-resani, and Manohar Paluri. Learning spatio-temporal features with 3d convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4489–4497, 2015.<br>[7] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on Computer Vision, pages 20–36. Springer, 2016.<br>[8] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, 2019.<br>[9] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recog-nition. In Proceedings of the IEEE international Conference on Computer Vision, pages 6202–6211, 2019.<br>[10] Jihoon Chung, Cheng hsin Wuu, Hsuan Ru Yang, Yu-Wing  Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos, 2020.<br>[11] Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, and Chi-Keung Tang. Semi-supervised few-shot atomic actionrecognition.arXiv preprint arXiv:2011.08410, 2020.  </p>
<h2 id="Todos"><a href="#Todos" class="headerlink" title="Todos"></a><strong>Todos</strong></h2><ol>
<li>Add more content.</li>
<li>Add more references.</li>
<li>The video is too small when using the new theme. Fix it. (Dongfeng)</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header">&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
    
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>








  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>

<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/04/30/main/"/>
    <url>/2021/04/30/main/</url>
    
    <content type="html"><![CDATA[<p><strong><h2 style="text-align:center;">Dense Token Supervision for Accelerating Transformer Training</h2></strong></p><p style="text-align:center;">    Shuqin Xie, Dongfeng Yu, Raymond Lau    <br>    Carnegie Mellon University</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h2><p>Transformer-based architectures are getting more and more attention in the computer vision field starting from its successful application in image classification[1,12]. The recent proposed Vision Transformer [1] (ViT) divides an image into a sequence of patches and considers them as input tokens to a transformer. The transformer architecture consists of multiple layers of self-attention and MLP. It also contains an extra “class token” that is responsible for generating the image-level prediction. At training time, a classification loss is applied only to the output of “class token”. An illustration of the ViT model is shown below.<br><img align="center" height="512" src="/images/ViT.jpg"></p><p>As pointed out in [1], ViT model requires huge amount of data to achieve comparable performance with CNN architectures. Although this problem is alleviated in later work [12] by distillating knowledge from pretrained CNNs, the training time is still much longer compared to regular CNN (1000 epochs v.s 100). We suspect the problem being only applying loss on one token’s output. Its token-wise nature raises the question of whether we can apply supervision on every tokens to serve purposes such as achieving better accuracy or more efficient training.</p><p>Our project explores this idea of dense token supervision for transformer on <strong>video action recognition</strong> and <strong>image classification tasks</strong>. We first consider the video action recognition task. FineGym[2] dataset provides both video-level labels and frame-level labels, which is pefectly suitable for our project as we can consider each frame as one token and naturally obtain labels for tokens. Our takeaway from this task is that applying dense token supervision can decrease training loss much faster. But since FineGym is small and is relatively easy, we don’t observe obvious performance difference after training for one epoch. We then move to image classification task, where larger datasets are available (e.g. ImageNet). Unlike the video experiment, there is no available labels for image patches. Here we use a pretrained CNN model to generate token labels for free. Our results show that applying dense token supervision to transformers makes it easier for the model to converge to satifying accuracy (less sensensitive to the learning rate), and accelerates the training when trained from scratch.</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a><strong>Related Work</strong></h2><p><strong>Vision Transformer</strong> DeTR [3], which is achieving similar performance with other state-of-the art object detection algorithms, is a novel architecture that composed of traditional CNN and transformer. ViT [1] is an image classifier based on transformer and comsumes an divided image of 16 x 16 words. DeiT [4] further improves ViT’s data efficiency by distilling knowledge using a CNN. [5] extends ViT from image classification to action recognition.<br><strong>Action Recognition</strong> Action recognition models take videos as input and predict the corresponding actions (labels). Recent works [6, 7, 8] utilize 2D CNNs structure to extract framewise features and further combined those with a temporal model. It is an improvement compared to the 3D convolution neural networks [9], which has a high complexity and thus computational expensive.<br><strong>Fine-grained Action Recognition</strong> State-of-the-art models have achieved great success in classical action recognition. The focus has been shifted to the fine-grained action recognitions [2,7]. For fine-grained action recogitions, some classes are similar in context but different in some sub-action levels. Compared to the traditional action recognition, models are required to differentiate those minor differences and learn the specific temporal and spatial features. [11] is one of the models, which is based on attention mechanism and semi-supervised learning.  </p><h2 id="Video-Classification-with-Frame-level-Labels"><a href="#Video-Classification-with-Frame-level-Labels" class="headerlink" title="Video Classification with Frame-level Labels"></a><strong>Video Classification with Frame-level Labels</strong></h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p><strong>FineGym</strong> [2] is an action recognition dataset which contains coarse-to-fine annotations with three levels of labels. The data source is based on the gymnastic videos, which are publicly available on YouTube. The top level corresponds to events such as balance beam or uneven bars. The second level and the third level corresponds to the sub-actions with decreasing granularity.  </p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>We extend vision transformer into video transformer as follows. Given a video with L frames, we divide each frame into 14x14 patches and feed them into the transformer architecture. With an extra class token, there are Lx14x14+1 tokens in total. Token features from the same frame are fused to generate a final logit for frame-level prediction. Since frame-level labels on FineGym is category label, we apply cross entropy loss on each frame logit. The architecture is shown below. To utilize pretrained image classification models, we initialize the position embedding by replicating pretrained model’s position embedding L times. The other weights are copied from pretrained models.<br><img align="center" width="1200" src="/images/video_transformer.png"></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>As we mentioned before, this dataset is too easy that ViT and our approach’s performance almost saturate after one epoch. In terms of top 1 accuracy, we did not observe any significant difference. But we found at training time, with dense token supervision, video classification loss decrease much faster than baseline. This leads to our image classification experiments. </p><center><table><thead><tr><th></th><th>Top-1 acc</th></tr></thead><tbody><tr><td>ViT</td><td>0.985</td></tr><tr><td>Ours</td><td>0.990</td></tr></tbody></table></center><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a><strong>Image Classification</strong></h2><h3 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a>Dataset</h3><p>Introduce ImageNet and Places365.</p><h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h3><p>Introduce our model.<br><img align="center" width="1024" height="512" src="/images/image.png"></p><h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p>Show the results.<br><img align="center" width="1024" height="512" src="/images/pretrained.png"><br><img align="center" width="1024" height="512" src="/images/scratch.png"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>Give an overall conclusion, and talk about future steps.</p><h2 id="Supplemental-Materials"><a href="#Supplemental-Materials" class="headerlink" title="Supplemental Materials"></a><strong>Supplemental Materials</strong></h2><h3 id="Presentation-Video"><a href="#Presentation-Video" class="headerlink" title="Presentation Video"></a>Presentation Video</h3><style>.embed-container {    position: relative;    padding-bottom: 56.25%;    height: 0;    overflow: hidden;    max-width: 100%;  }  .embed-container iframe, .embed-container object, .embed-container embed {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;  }  </style><div class="embed-container"><iframe src="https://www.youtube.com/embed/Ks_YPSJZAGE" allowfullscreen frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe></div><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p><a href="/files/dense_token.zip">dense_token.zip</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h2><p>[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.<br>[2] Shao, Dian, et al. “Finegym: A hierarchical video dataset for fine-grained action understanding.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and SergeyZagoruyko. End-to-end object detection with trans-formers. In European Conference on Computer Vi-sion, pages 213–229. Springer, 2020.<br>[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-cisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers &amp; distillation through attention. arXiv preprint arXiv:2012.12877, 2020.<br>[5] Daniel Neimark, Omri Bar, Maya Zohar, and DotanAsselmann.Video transformer network. arXiv preprint, arXiv:2102.00719, 2021.<br>[6] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on Computer Vision, pages 20–36. Springer, 2016.<br>[7] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, 2019.<br>[8] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recog-nition. In Proceedings of the IEEE international Conference on Computer Vision, pages 6202–6211, 2019.<br>[9] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-resani, and Manohar Paluri. Learning spatio-temporal features with 3d convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4489–4497, 2015.<br>[10] Jihoon Chung, Cheng hsin Wuu, Hsuan Ru Yang, Yu-Wing  Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos, 2020.<br>[11] Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, and Chi-Keung Tang. Semi-supervised few-shot atomic actionrecognition.arXiv preprint arXiv:2011.08410, 2020.<br>[12] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention. arXiv preprint arXiv:2012.12877, 2020.</p><h2 id="Todos"><a href="#Todos" class="headerlink" title="Todos"></a><strong>Todos</strong></h2><ol><li>Add more content.</li><li>Add more references.</li><li>Check <a href="https://piazza.com/class/kk5qztnow8t2p9?cid=317">https://piazza.com/class/kk5qztnow8t2p9?cid=317</a></li><li>Check <a href="https://piazza.com/class/kk5qztnow8t2p9?cid=303">https://piazza.com/class/kk5qztnow8t2p9?cid=303</a></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
